{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "420e1df3",
      "metadata": {
        "id": "420e1df3"
      },
      "source": [
        "# Anima LoRA Trainer — Quick README (v4)\n",
        "\n",
        "> **GitHub repo:** [citronlegacy/citron-colab-anima-lora-trainer](https://github.com/citronlegacy/citron-colab-anima-lora-trainer)\n",
        "\n",
        "## Purpose\n",
        "\n",
        "This notebook trains a LoRA for the [Anima](https://huggingface.co/circlestone-labs/Anima) diffusion model using [kohya-ss/sd-scripts](https://github.com/kohya-ss/sd-scripts). It is designed to run entirely inside Google Colab.\n",
        "\n",
        "## Dataset format (flat)\n",
        "\n",
        "- Use a flat directory (no nested subfolders).\n",
        "- Each image must have a caption file with the exact same basename and a `.txt` extension.\n",
        "- Supported image formats: `.jpg`, `.jpeg`, `.png`, `.webp`, `.bmp`, `.gif`.\n",
        "\n",
        "### Example structure:\n",
        "\n",
        "```\n",
        "my_dataset/\n",
        "  image001.png\n",
        "  image001.txt    # caption: tag-based, comma-separated tags\n",
        "  image002.jpg\n",
        "  image002.txt\n",
        "  image003.webp\n",
        "  image003.txt\n",
        "```\n",
        "\n",
        "### Tags format\n",
        "\n",
        "- Use tag-style captions (comma-separated tags). No special tokens required.\n",
        "- Example caption: `mycharname, 1girl, long blonde hair, blue eyes, high quality, detailed`\n",
        "\n",
        "## Colab quick usage\n",
        "\n",
        "1. Install dependencies (run the Setup cell).\n",
        "2. (Optional) Enable the `mount_drive` checkbox in the Setup cell to mount Google Drive before downloading models.\n",
        "3. Download models (run the Setup cell). Models are stored in `/content/models/anima`.\n",
        "4. Set `project_name`, `image_directory` (flat folder), `output_directory`, and hyperparameters in the Training Settings cell.\n",
        "5. Run the Training cell to generate TOML configs and start training.\n",
        "\n",
        "## ⚠️ Colab runtime limit — target < 1000 training steps\n",
        "\n",
        "In testing, reaching 1000 training steps takes **over 4 hours** on Colab and often causes the session to disconnect before completion. Keep your total training steps under 1000 to finish reliably in a single session.\n",
        "\n",
        "**Step calculation:**\n",
        "\n",
        "$$steps\\_per\\_epoch = \\left\\lceil \\frac{N_{images} \\times repeats}{batch\\_size \\times grad\\_accum} \\right\\rceil$$\n",
        "\n",
        "$$total\\_steps = steps\\_per\\_epoch \\times epochs$$\n",
        "\n",
        "The Training Settings cell will automatically calculate and warn you if your settings exceed 1000 steps.\n",
        "\n",
        "## Behavior and defaults\n",
        "\n",
        "- Single-project execution: the notebook trains one LoRA per run (no batch processing).\n",
        "- No automatic resume: if training is interrupted, checkpoints remain in the output folder; you may manually resume via sd-scripts if desired.\n",
        "- Defaults exposed to users: `Epochs=10`, `Dim=20`, `Alpha=20`, `Resolution=768`, `Learning Rate=0.0001`, `Caption Dropout=0.1`.\n",
        "- Models: download to `/content/models` (do not assume Drive persistence unless you save there).\n",
        "\n",
        "## Google Drive integration note\n",
        "\n",
        "For consistent Google Drive integration, always work inside the Drive root `lora_training` directory. Use these three folders under your Drive root for predictable behavior and easy syncing:\n",
        "\n",
        "- `lora_training/datasets`\n",
        "- `lora_training/configs`\n",
        "- `lora_training/output`\n",
        "\n",
        "Enable the `mount_drive` checkbox in the Setup cell to mount Drive before model downloads. Then set `image_directory` and `output_directory` to paths under `/content/drive/MyDrive/`.\n",
        "\n",
        "## Troubleshooting\n",
        "\n",
        "- CUDA OOM: reduce `network_dim` or `resolution` (try `network_dim=8` and/or `resolution=512`).\n",
        "- NaN loss: ensure PyTorch >= 2.5 and lower the learning rate.\n",
        "- \"No images found\": verify captions are `.txt` files and images are not named only `.txt`.\n",
        "- Too many steps / Colab disconnect: lower `repeats`, `max_train_epochs`, or use fewer images per run.\n",
        "\n",
        "## Saving and checkpoints\n",
        "\n",
        "- Trained LoRAs and epoch checkpoints are written to the `output_directory` you set (or Drive default when mounted).\n",
        "- The notebook will create `/lora_training/output` inside your Drive mount when `mount_drive=True` and no custom `output_directory` is provided.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d756691",
      "metadata": {
        "id": "8d756691"
      },
      "outputs": [],
      "source": [
        "#@title Setup Cell — Install sd-scripts, dependencies, and download Anima models\n",
        "# Run this cell first in Colab.\n",
        "\n",
        "#@markdown Mount Google Drive (optional). If enabled, the notebook will mount before downloading models.\n",
        "mount_drive = False #@param {type:\"boolean\"}\n",
        "if mount_drive:\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        print(\"✓ Google Drive mounted at /content/drive\")\n",
        "    except Exception as e:\n",
        "        print(\"⚠ Could not mount Google Drive:\", e)\n",
        "\n",
        "# Clone and install sd-scripts (idempotent)\n",
        "!if [ -d /content/sd-scripts ]; then echo 'sd-scripts already cloned'; else git clone https://github.com/kohya-ss/sd-scripts.git /content/sd-scripts; fi\n",
        "%cd /content/sd-scripts\n",
        "!python -m pip install --upgrade pip\n",
        "!pip install -r requirements.txt\n",
        "!pip install toml\n",
        "\n",
        "# Verify installation\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "\n",
        "\n",
        "# Create model directories and download models to /content/models (idempotent)\n",
        "!mkdir -p /content/models/anima/dit\n",
        "!mkdir -p /content/models/anima/text_encoder\n",
        "!mkdir -p /content/models/anima/vae\n",
        "\n",
        "print(\"Downloading Anima DiT model (4.18 GB)...\")\n",
        "!wget -c --show-progress -O /content/models/anima/dit/anima-preview.safetensors \\\n",
        "  https://huggingface.co/circlestone-labs/Anima/resolve/main/split_files/diffusion_models/anima-preview.safetensors\n",
        "\n",
        "print(\"Downloading Qwen3 text encoder (1.19 GB)...\")\n",
        "!wget -c --show-progress -O /content/models/anima/text_encoder/qwen_3_06b_base.safetensors \\\n",
        "  https://huggingface.co/circlestone-labs/Anima/resolve/main/split_files/text_encoders/qwen_3_06b_base.safetensors\n",
        "\n",
        "print(\"Downloading Qwen-Image VAE (254 MB)...\")\n",
        "!wget -c --show-progress -O /content/models/anima/vae/qwen_image_vae.safetensors \\\n",
        "  https://huggingface.co/circlestone-labs/Anima/resolve/main/split_files/vae/qwen_image_vae.safetensors\n",
        "\n",
        "print(\"\\n✓ All setup steps finished. Proceed to the settings cell.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3530dd9e",
      "metadata": {
        "id": "3530dd9e"
      },
      "outputs": [],
      "source": [
        "#@title Unzip a dataset\n",
        "zip_file_path = \"/content/my_dataset.zip\" #@param {type:\"string\"}\n",
        "\n",
        "import os\n",
        "\n",
        "if os.path.exists(zip_file_path):\n",
        "    print(f\"Unzipping {zip_file_path}...\")\n",
        "    destination_dir = os.path.dirname(zip_file_path)\n",
        "    !unzip -o -q \"{zip_file_path}\" -d \"{destination_dir}\"\n",
        "    print(\"Unzipping complete.\")\n",
        "else:\n",
        "    print(f\"Error: Zip file not found at {zip_file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c013cd8",
      "metadata": {
        "id": "8c013cd8"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# USER SETTINGS - Modify these to configure your training\n",
        "# ============================================================\n",
        "\n",
        "#@title Training Settings\n",
        "#@markdown In this cell you will define the training settings. The cell also estimates your total training steps and warns you if they exceed 1000 (which risks a Colab timeout). Training runs in the next cell.\n",
        "\n",
        "#@markdown ## Project Configuration\n",
        "project_name = \"my_lora\" #@param {type:\"string\"}\n",
        "# Set `image_directory` to a path under /content/drive if you mounted Drive in the Setup cell.\n",
        "image_directory = \"/content/drive/MyDrive/lora_training/datasets/my_dataset\" #@param {type:\"string\"}\n",
        "output_directory = \"/content/drive/MyDrive/lora_training/output\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ## Training parameters\n",
        "network_dim = 20 #@param {type:\"integer\"}\n",
        "network_alpha = 20 #@param {type:\"integer\"}\n",
        "learning_rate = 0.0001 #@param {type:\"number\"}\n",
        "max_train_epochs = 10 #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown ## Dataset Settings\n",
        "resolution = 768 #@param {type:\"integer\"}\n",
        "repeats = 5 #@param {type:\"integer\"}\n",
        "caption_dropout = 0.1 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown **Tip**: Higher `network_dim` = more capacity but requires more VRAM\n",
        "#@markdown\n",
        "#@markdown **Tip**: `repeats` × number of images = steps per epoch\n",
        "\n",
        "# Model paths (should not need to change these)\n",
        "DIT_MODEL = \"/content/models/anima/dit/anima-preview.safetensors\"\n",
        "QWEN3_MODEL = \"/content/models/anima/text_encoder/qwen_3_06b_base.safetensors\"\n",
        "VAE_MODEL = \"/content/models/anima/vae/qwen_image_vae.safetensors\"\n",
        "\n",
        "# Display settings summary\n",
        "print(\"=\"*60)\n",
        "print(\"TRAINING CONFIGURATION\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Project Name:      {project_name}\")\n",
        "print(f\"Image Directory:   {image_directory}\")\n",
        "print(f\"Output Directory:  {output_directory}\")\n",
        "print(f\"Network Dim:       {network_dim}\")\n",
        "print(f\"Network Alpha:     {network_alpha}\")\n",
        "print(f\"Learning Rate:     {learning_rate}\")\n",
        "print(f\"Max Epochs:        {max_train_epochs}\")\n",
        "print(f\"Resolution:        {resolution}px\")\n",
        "print(f\"Repeats:           {repeats}\")\n",
        "print(f\"Caption Dropout:   {caption_dropout}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ── Step estimator ──────────────────────────────────────────\n",
        "import os as _os\n",
        "import math as _math\n",
        "\n",
        "def estimate_steps(image_dir=None, num_images=None, repeats=5, epochs=10,\n",
        "                   batch_size=1, grad_accum=1):\n",
        "    \"\"\"Return (steps_per_epoch, total_steps, num_images).\"\"\"\n",
        "    if image_dir and _os.path.exists(image_dir) and num_images is None:\n",
        "        files = _os.listdir(image_dir)\n",
        "        num_images = len([f for f in files\n",
        "                          if not f.lower().endswith('.txt')\n",
        "                          and _os.path.isfile(_os.path.join(image_dir, f))])\n",
        "    if num_images is None:\n",
        "        raise ValueError('Provide image_dir or num_images')\n",
        "    spe = _math.ceil((num_images * int(repeats)) / (int(batch_size) * int(grad_accum)))\n",
        "    return spe, spe * int(epochs), num_images\n",
        "\n",
        "print()\n",
        "print(\"── Step Estimate ──────────────────────────────────────\")\n",
        "try:\n",
        "    _spe, _tot, _n = estimate_steps(\n",
        "        image_dir=image_directory,\n",
        "        repeats=repeats,\n",
        "        epochs=max_train_epochs\n",
        "    )\n",
        "    print(f\"  Images found:        {_n}\")\n",
        "    print(f\"  Steps per epoch:     {_spe}  ({_n} images × {repeats} repeats)\")\n",
        "    print(f\"  Total steps:         {_tot}  ({_spe} × {max_train_epochs} epochs)\")\n",
        "    if _tot > 1000:\n",
        "        print()\n",
        "        print(\"  ⚠️  WARNING: Total steps exceed 1000!\")\n",
        "        print(\"  In testing, 1000 steps takes 4+ hours and risks a Colab disconnect.\")\n",
        "        print(\"  Suggestions to reduce steps:\")\n",
        "        print(f\"    • Lower epochs   (current: {max_train_epochs})  →  try {max(_math.ceil(_tot / (_spe * 2)), 1)}\")\n",
        "        print(f\"    • Lower repeats  (current: {repeats})  →  try {max(_math.ceil(1000 / (_n * max_train_epochs)), 1)}\")\n",
        "        print(\"    • Use fewer images per run (split your dataset across sessions)\")\n",
        "    else:\n",
        "        print(\"  ✓ Steps look good — within the recommended < 1000 limit.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"  (image_directory not found yet — steps will be checked at runtime)\")\n",
        "except Exception as _e:\n",
        "    print(f\"  Could not estimate steps: {_e}\")\n",
        "print(\"──────────────────────────────────────────────────────\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c17bd902",
      "metadata": {
        "id": "c17bd902"
      },
      "outputs": [],
      "source": [
        "#@title Training Cell — Generate configs and run training\n",
        "# Run this cell after the Setup cell and the Training Settings cell.\n",
        "#\n",
        "# Uses subprocess.Popen for real-time streaming output so progress bars\n",
        "# and loss values appear live in Colab. On failure, prints recent dmesg\n",
        "# to help diagnose OOM kills.\n",
        "\n",
        "import os\n",
        "import shlex\n",
        "import toml\n",
        "from datetime import datetime\n",
        "import subprocess\n",
        "import math\n",
        "\n",
        "# Ensure terminal-like output for tqdm/rich progress bars\n",
        "os.environ.setdefault('PYTHONUNBUFFERED', '1')\n",
        "os.environ.setdefault('TERM', 'xterm')\n",
        "os.environ.setdefault('FORCE_TQDM', '1')\n",
        "\n",
        "# Pull model paths from settings cell (with safe fallbacks)\n",
        "DIT_MODEL   = globals().get('DIT_MODEL',   '/content/models/anima/dit/anima-preview.safetensors')\n",
        "QWEN3_MODEL = globals().get('QWEN3_MODEL', '/content/models/anima/text_encoder/qwen_3_06b_base.safetensors')\n",
        "VAE_MODEL   = globals().get('VAE_MODEL',   '/content/models/anima/vae/qwen_image_vae.safetensors')\n",
        "\n",
        "CONFIG_DIR = '/content/lora_training/configs'\n",
        "os.makedirs(CONFIG_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "def create_training_config(project_name, output_dir, dit_model_path, qwen3_model_path, vae_model_path,\n",
        "                           network_dim=20, network_alpha=20, learning_rate=1e-4, max_train_epochs=10):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    current_date = datetime.now().strftime('%Y-%m-%d')\n",
        "    config_filename = f\"{project_name}_training_config_{current_date}.toml\"\n",
        "    config_path = os.path.join(CONFIG_DIR, config_filename)\n",
        "\n",
        "    training_config = {\n",
        "        'pretrained_model_name_or_path': dit_model_path,\n",
        "        'qwen3': qwen3_model_path,\n",
        "        'vae': vae_model_path,\n",
        "        'network_module': 'networks.lora_anima',\n",
        "        'network_dim': int(network_dim),\n",
        "        'network_alpha': int(network_alpha),\n",
        "        'network_train_unet_only': True,\n",
        "        'learning_rate': float(learning_rate),\n",
        "        'optimizer_type': 'AdamW8bit',\n",
        "        'optimizer_args': ['weight_decay=0.1', 'betas=[0.9, 0.99]'],\n",
        "        'lr_scheduler': 'cosine_with_restarts',\n",
        "        'lr_scheduler_num_cycles': 1,\n",
        "        'lr_warmup_steps': 100,\n",
        "        'max_train_epochs': int(max_train_epochs),\n",
        "        'train_batch_size': 1,\n",
        "        'gradient_accumulation_steps': 1,\n",
        "        'max_grad_norm': 1.0,\n",
        "        'seed': 42,\n",
        "        'timestep_sampling': 'sigmoid',\n",
        "        'discrete_flow_shift': 1.0,\n",
        "        'qwen3_max_token_length': 512,\n",
        "        't5_max_token_length': 512,\n",
        "        'mixed_precision': 'bf16',\n",
        "        'gradient_checkpointing': True,\n",
        "        'cache_latents': True,\n",
        "        'cache_text_encoder_outputs': True,\n",
        "        'vae_chunk_size': 64,\n",
        "        'vae_disable_cache': True,\n",
        "        'output_dir': output_dir,\n",
        "        'output_name': project_name,\n",
        "        'save_model_as': 'safetensors',\n",
        "        'save_precision': 'bf16',\n",
        "        'save_every_n_epochs': 1,\n",
        "        'save_last_n_epochs': 4,\n",
        "        'shuffle_caption': False,\n",
        "        'caption_extension': '.txt',\n",
        "        'noise_offset': 0.03,\n",
        "        'multires_noise_discount': 0.3,\n",
        "        'training_comment': f'Anima LoRA - {datetime.now().strftime(\"%Y-%m-%d\")}',\n",
        "    }\n",
        "\n",
        "    with open(config_path, 'w') as f:\n",
        "        toml.dump(training_config, f)\n",
        "\n",
        "    print(f\"\\u2713 Created training config: {config_filename}\")\n",
        "    return config_path\n",
        "\n",
        "\n",
        "def create_dataset_config(project_name, image_dir, resolution=768, repeats=10, caption_dropout_rate=0.1):\n",
        "    if not os.path.exists(image_dir):\n",
        "        raise FileNotFoundError(f\"Image directory not found: {image_dir}\")\n",
        "\n",
        "    all_files = os.listdir(image_dir)\n",
        "    image_files = [f for f in all_files\n",
        "                   if not f.lower().endswith('.txt')\n",
        "                   and os.path.isfile(os.path.join(image_dir, f))]\n",
        "    if len(image_files) == 0:\n",
        "        raise ValueError(f\"No image files found in {image_dir}\")\n",
        "\n",
        "    current_date = datetime.now().strftime('%Y-%m-%d')\n",
        "    config_filename = f\"{project_name}_dataset_config_{current_date}.toml\"\n",
        "    config_path = os.path.join(CONFIG_DIR, config_filename)\n",
        "\n",
        "    dataset_config = {\n",
        "        'general': {\n",
        "            'resolution': int(resolution),\n",
        "            'enable_bucket': True,\n",
        "            'bucket_no_upscale': False,\n",
        "            'bucket_reso_steps': 64,\n",
        "            'min_bucket_reso': 256,\n",
        "            'max_bucket_reso': 4096,\n",
        "        },\n",
        "        'datasets': [\n",
        "            {\n",
        "                'resolution': int(resolution),\n",
        "                'subsets': [\n",
        "                    {\n",
        "                        'num_repeats': int(repeats),\n",
        "                        'image_dir': image_dir,\n",
        "                        'caption_extension': '.txt',\n",
        "                        'caption_dropout_rate': float(caption_dropout_rate),\n",
        "                    }\n",
        "                ]\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    with open(config_path, 'w') as f:\n",
        "        toml.dump(dataset_config, f)\n",
        "\n",
        "    print(f\"\\u2713 Created dataset config: {config_filename}\")\n",
        "    return config_path\n",
        "\n",
        "\n",
        "def train_lora_simple(cmd_list):\n",
        "    \"\"\"Run training command with real-time streaming output for Colab.\"\"\"\n",
        "    import sys\n",
        "    print(\"Executing:\", ' '.join(shlex.quote(c) for c in cmd_list))\n",
        "    print()\n",
        "\n",
        "    process = subprocess.Popen(\n",
        "        cmd_list,\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT,\n",
        "        universal_newlines=True,\n",
        "        bufsize=1\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        for line in iter(process.stdout.readline, ''):\n",
        "            if line:\n",
        "                print(line, end='', flush=True)\n",
        "                sys.stdout.flush()\n",
        "    except KeyboardInterrupt:\n",
        "        process.kill()\n",
        "        process.wait()\n",
        "        raise\n",
        "\n",
        "    process.stdout.close()\n",
        "    exit_code = process.wait()\n",
        "\n",
        "    if exit_code == 0:\n",
        "        print(\"\\n\\u2713 Training completed successfully!\")\n",
        "        return True\n",
        "    else:\n",
        "        print(f\"\\n\\u2717 Training failed (exit code: {exit_code})\")\n",
        "        try:\n",
        "            print('\\n--- Recent kernel messages (dmesg) ---')\n",
        "            dmesg = subprocess.check_output(['dmesg', '-T'], stderr=subprocess.STDOUT, text=True)\n",
        "            tail = '\\n'.join(dmesg.splitlines()[-80:])\n",
        "            print(tail)\n",
        "            if any(t in tail for t in ('Out of memory', 'Killed process', 'oom_reaper', 'OOM')):\n",
        "                print('\\n\\U0001f4a1 Hint: OOM detected. Try: network_dim=8, resolution=512')\n",
        "        except Exception:\n",
        "            pass\n",
        "        return False\n",
        "\n",
        "\n",
        "def main():\n",
        "    g = globals()\n",
        "\n",
        "    # Optionally mount Drive\n",
        "    if g.get('mount_drive', False):\n",
        "        try:\n",
        "            from google.colab import drive\n",
        "            drive.mount('/content/drive')\n",
        "            print(\"\\u2713 Google Drive mounted at /content/drive\")\n",
        "        except Exception as e:\n",
        "            print(\"\\u26a0 Could not mount Google Drive:\", e)\n",
        "\n",
        "    project_name     = g.get('project_name', 'my_lora')\n",
        "    image_directory  = g.get('image_directory', '/content/my_training_images')\n",
        "    output_directory = g.get('output_directory', '/content/lora_output')\n",
        "    network_dim      = g.get('network_dim', 20)\n",
        "    network_alpha    = g.get('network_alpha', 20)\n",
        "    learning_rate    = g.get('learning_rate', 1e-4)\n",
        "    max_train_epochs = g.get('max_train_epochs', 10)\n",
        "    resolution       = g.get('resolution', 768)\n",
        "    repeats          = g.get('repeats', 10)\n",
        "    caption_dropout  = g.get('caption_dropout', 0.1)\n",
        "\n",
        "    # Validate image dir\n",
        "    if not os.path.exists(image_directory):\n",
        "        raise SystemExit(f\"Image directory not found: {image_directory}\")\n",
        "\n",
        "    # Re-run step check at training time (catches changes since settings cell)\n",
        "    all_files = os.listdir(image_directory)\n",
        "    n_images = len([f for f in all_files\n",
        "                    if not f.lower().endswith('.txt')\n",
        "                    and os.path.isfile(os.path.join(image_directory, f))])\n",
        "    spe = math.ceil((n_images * int(repeats)) / 1)\n",
        "    total_steps = spe * int(max_train_epochs)\n",
        "    print(f\"\\nStep check: {n_images} images × {repeats} repeats × {max_train_epochs} epochs = {total_steps} steps\")\n",
        "    if total_steps > 1000:\n",
        "        print(\"  \\u26a0\\ufe0f  WARNING: Total steps exceed 1000 — this run risks a Colab timeout (4+ hours).\")\n",
        "        print(\"  Consider reducing epochs or repeats before continuing.\")\n",
        "\n",
        "    # Validate models\n",
        "    print(\"\\nValidating models...\")\n",
        "    ok = True\n",
        "    for name, path in [('DiT', DIT_MODEL), ('Qwen3', QWEN3_MODEL), ('VAE', VAE_MODEL)]:\n",
        "        if os.path.exists(path):\n",
        "            print(f\"  \\u2713 {name} found\")\n",
        "        else:\n",
        "            print(f\"  \\u2717 {name} missing: {path}\")\n",
        "            ok = False\n",
        "    if not ok:\n",
        "        raise SystemExit(\"Required models missing — run the setup cell first.\")\n",
        "\n",
        "    # Create configs\n",
        "    train_cfg = create_training_config(\n",
        "        project_name, output_directory, DIT_MODEL, QWEN3_MODEL, VAE_MODEL,\n",
        "        network_dim=network_dim, network_alpha=network_alpha,\n",
        "        learning_rate=learning_rate, max_train_epochs=max_train_epochs\n",
        "    )\n",
        "    dataset_cfg = create_dataset_config(\n",
        "        project_name, image_directory,\n",
        "        resolution=resolution, repeats=repeats, caption_dropout_rate=caption_dropout\n",
        "    )\n",
        "\n",
        "    # Build accelerate command\n",
        "    cmd = [\n",
        "        'accelerate', 'launch',\n",
        "        '--num_cpu_threads_per_process', str(g.get('NUM_CPU_THREADS_PER_PROCESS', 1)),\n",
        "        '/content/sd-scripts/anima_train_network.py',\n",
        "        '--config_file', train_cfg,\n",
        "        '--dataset_config', dataset_cfg\n",
        "    ]\n",
        "\n",
        "    success = train_lora_simple(cmd)\n",
        "\n",
        "    print('\\n=== Training Summary ===')\n",
        "    print(f'Project: {project_name}')\n",
        "    print('Status:', '\\u2713 Success' if success else '\\u2717 Failed')\n",
        "    if success:\n",
        "        print(f'Trained LoRA saved to: {output_directory}')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    try:\n",
        "        main()\n",
        "    except Exception as e:\n",
        "        print('Fatal error:', e)\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
